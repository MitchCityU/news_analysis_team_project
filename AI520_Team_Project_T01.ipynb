{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a7a7eb",
   "metadata": {},
   "source": [
    "# AI520: Natural Processing Language for Artificial Intelligence\n",
    "Term: Summer 2025 \\\n",
    "Author: David Hiltzman \\\n",
    "Assignment: Team Project \\\n",
    "Team 01 \\\n",
    "Authors: \\\n",
    "Mitch Fade, David Hiltzman, Tyler Kepler, Jeff Nelson\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d665366",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "This project will create a false news story detector using a publicly available Kaggle labeled dataset with known real and false stories. Data will first be preprocessed, and feature extraction will be performed. Classical machine learning (ML) methods and deep learning methods will be used to create classifiers of the preprocessed dataset and evaluated for effectiveness. Additionally, the explainability of the results will be analyzed to show which words contributed the most to the predictions. A false news detector may help bring clarity in a world of ever-growing misinformation and may give interesting results on which words are most common in fake news stories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e79f0cb",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acb13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5630a",
   "metadata": {},
   "source": [
    "## Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess a given text string by normalizing, tokenizing, \n",
    "    removing stop words, and lemmatizing.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed text string\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]  # Remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]   # Lemmatize\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalize column names by stripping whitespace and converting to lowercase.\"\"\"\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d6be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_date_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find a likely date column, parse to datetime, and write back to a unified 'date' column.\n",
    "    If no date-like column exists, create an empty 'date' column.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        \"date\", \"published\", \"publish_date\", \"publication_date\",\n",
    "        \"pub_date\", \"created_at\", \"time\", \"timestamp\"\n",
    "    ]\n",
    "    date_col = next((c for c in candidates if c in df.columns), None)\n",
    "    \n",
    "    if date_col is None:\n",
    "        df[\"date\"] = pd.NaT\n",
    "    else:\n",
    "        df[\"date\"] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    \n",
    "    # Write dates as ISO strings (YYYY-MM-DD); keep NaN if unknown\n",
    "    df[\"date\"] = df[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebbfe10",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca33b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df_true = pd.read_csv(\"True.csv\")\n",
    "df_fake = pd.read_csv(\"Fake.csv\")\n",
    "\n",
    "# Normalize headers to avoid case/space issues\n",
    "df_true = normalize_columns(df_true)\n",
    "df_fake = normalize_columns(df_fake)\n",
    "\n",
    "# Make sure we have a usable 'date' column in both\n",
    "df_true = ensure_date_column(df_true)\n",
    "df_fake = ensure_date_column(df_fake)\n",
    "\n",
    "print(\"Unprocessed True dataset shape:\", df_true.shape)\n",
    "print(\"Unprocessed Fake dataset shape:\", df_fake.shape)\n",
    "print(\"\\nTrue dataset columns:\", df_true.columns.tolist())\n",
    "print(\"Fake dataset columns:\", df_fake.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c8850",
   "metadata": {},
   "source": [
    "## Data Validation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffd8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guard for missing title/text columns\n",
    "required = {\"title\", \"text\"}\n",
    "missing_true = required - set(df_true.columns)\n",
    "missing_fake = required - set(df_fake.columns)\n",
    "\n",
    "if missing_true:\n",
    "    raise KeyError(f\"True.csv missing required columns: {missing_true}\")\n",
    "if missing_fake:\n",
    "    raise KeyError(f\"Fake.csv missing required columns: {missing_false}\")\n",
    "\n",
    "# Apply preprocessing to title and text columns\n",
    "print(\"Preprocessing text data...\")\n",
    "df_true[\"processed_title\"] = df_true[\"title\"].astype(str).apply(preprocess_text)\n",
    "df_fake[\"processed_title\"] = df_fake[\"title\"].astype(str).apply(preprocess_text)\n",
    "df_true[\"processed_text\"] = df_true[\"text\"].astype(str).apply(preprocess_text)\n",
    "df_fake[\"processed_text\"] = df_fake[\"text\"].astype(str).apply(preprocess_text)\n",
    "\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893eb283",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data and Display Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269af65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data\n",
    "df_true[[\"processed_title\", \"processed_text\", \"date\"]].to_csv(\"preprocessed_True.csv\", index=False)\n",
    "df_fake[[\"processed_title\", \"processed_text\", \"date\"]].to_csv(\"preprocessed_Fake.csv\", index=False)\n",
    "\n",
    "print(\"Processed True dataset sample:\")\n",
    "print(df_true[[\"processed_title\", \"processed_text\", \"date\"]].head())\n",
    "print(\"\\nProcessed Fake dataset sample:\")\n",
    "print(df_fake[[\"processed_title\", \"processed_text\", \"date\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69315290",
   "metadata": {},
   "source": [
    "## Prepare Data for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84dedd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "df_fake = pd.read_csv(\"preprocessed_Fake.csv\")\n",
    "df_true = pd.read_csv(\"preprocessed_True.csv\")\n",
    "\n",
    "# Label the data\n",
    "df_fake['label'] = 0  # fake news\n",
    "df_true['label'] = 1  # true news\n",
    "\n",
    "# Combine the datasets\n",
    "df_combined = pd.concat([df_fake, df_true], ignore_index=True)\n",
    "\n",
    "# Combine title and text for feature extraction\n",
    "df_combined['combined'] = df_combined['processed_title'].fillna('') + ' ' + df_combined['processed_text'].fillna('')\n",
    "\n",
    "print(f\"Combined dataset shape: {df_combined.shape}\")\n",
    "print(f\"Label distribution:\\n{df_combined['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1d4ad",
   "metadata": {},
   "source": [
    "## Feature Extraction with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform data\n",
    "X = vectorizer.fit_transform(df_combined['combined'])  # Feature matrix\n",
    "y = df_combined['label']  # Target labels\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", X.shape)\n",
    "print(\"Sample features:\", vectorizer.get_feature_names_out()[:20])\n",
    "print(\"Feature matrix sparsity:\", (X != 0).nnz / (X.shape[0] * X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f0cbf",
   "metadata": {},
   "source": [
    "## Classifier 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c6370",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LOGISTIC REGRESSION CLASSIFIER\")\n",
    "\n",
    "# Create training/testing split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f82b83",
   "metadata": {},
   "source": [
    "## Classifier 2 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e02f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RANDOM FOREST CLASSIFIER\")\n",
    "\n",
    "# Create training/testing split (same random state for fair comparison)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,       # number of trees\n",
    "    random_state=42,        # for reproducibility\n",
    "    n_jobs=-1               # use all CPU cores\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(\"Random Forest Results:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28b6e33",
   "metadata": {},
   "source": [
    "## Classifier 3 - Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a797b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LINEAR SVM CLASSIFIER\")\n",
    "\n",
    "# Create training/testing split (same random state for fair comparison)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Linear SVM pipeline with scaling\n",
    "svm_model = make_pipeline(\n",
    "    StandardScaler(with_mean=False),  # Don't center sparse matrices\n",
    "    LinearSVC(dual=\"auto\", C=1.0, tol=1e-3, max_iter=5000, random_state=42)\n",
    ")\n",
    "\n",
    "# Train model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print(\"Linear SVM Results:\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9265756a",
   "metadata": {},
   "source": [
    "## Model Comparison and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbce668",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"All models trained on the same train/test split (random_state=42)\")\n",
    "print(\"Dataset size:\", df_combined.shape[0], \"samples\")\n",
    "print(\"Feature dimensions:\", X.shape[1], \"TF-IDF features\")\n",
    "print(\"Train/Test split: 80/20\")\n",
    "print(\"\\nRerun the classification cells above to see detailed results for each model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai520_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
