{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a7a7eb",
   "metadata": {},
   "source": [
    "# AI520: Natural Processing Language for Artificial Intelligence\n",
    "Term: Summer 2025 \\\n",
    "Author: David Hiltzman \\\n",
    "Assignment: Team Project \\\n",
    "Team 01 \\\n",
    "Authors: \\\n",
    "Mitch Fade, David Hiltzman, Tyler Kepler, Jeff Nelson\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d665366",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "This project will create a false news story detector using a publicly available Kaggle labeled dataset with known real and false stories. Data will first be preprocessed, and feature extraction will be performed. Classical machine learning (ML) methods and deep learning methods will be used to create classifiers of the preprocessed dataset and evaluated for effectiveness. Additionally, the explainability of the results will be analyzed to show which words contributed the most to the predictions. A false news detector may help bring clarity in a world of ever-growing misinformation and may give interesting results on which words are most common in fake news stories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e79f0cb",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8acb13af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/david/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/david/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c08918a",
   "metadata": {},
   "source": [
    "## Download Kaggle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "297c9c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created/verified data directory: ./data\n",
      "Path to dataset files: /home/david/.cache/kagglehub/datasets/clmentbisaillon/fake-and-real-news-dataset/versions/1\n",
      "Downloaded files: ['Fake.csv', 'True.csv']\n",
      "True file candidates: ['True.csv']\n",
      "Fake file candidates: ['Fake.csv']\n",
      "✓ Copied True.csv to ./data/True.csv\n",
      "✓ Copied Fake.csv to ./data/Fake.csv\n",
      "✓ Dataset files ready:\n",
      "  - ./data/True.csv\n",
      "  - ./data/Fake.csv\n"
     ]
    }
   ],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "data_dir = \"./data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "print(f\"✓ Created/verified data directory: {data_dir}\")\n",
    "\n",
    "# Download dataset using kagglehub\n",
    "try:   \n",
    "    # Download latest version\n",
    "    path = kagglehub.dataset_download(\"clmentbisaillon/fake-and-real-news-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    \n",
    "    # List files in the downloaded directory\n",
    "    downloaded_files = os.listdir(path)\n",
    "    print(\"Downloaded files:\", downloaded_files)\n",
    "    \n",
    "    # Look for True.csv and Fake.csv (or similar names)\n",
    "    true_file_candidates = [f for f in downloaded_files if 'true' in f.lower() and f.endswith('.csv')]\n",
    "    fake_file_candidates = [f for f in downloaded_files if 'fake' in f.lower() and f.endswith('.csv')]\n",
    "    \n",
    "    print(f\"True file candidates: {true_file_candidates}\")\n",
    "    print(f\"Fake file candidates: {fake_file_candidates}\")\n",
    "    \n",
    "    # Copy files to our data directory\n",
    "    if true_file_candidates and fake_file_candidates:\n",
    "        true_source = os.path.join(path, true_file_candidates[0])\n",
    "        fake_source = os.path.join(path, fake_file_candidates[0])\n",
    "        \n",
    "        true_dest = os.path.join(data_dir, \"True.csv\")\n",
    "        fake_dest = os.path.join(data_dir, \"Fake.csv\")\n",
    "        \n",
    "        shutil.copy2(true_source, true_dest)\n",
    "        shutil.copy2(fake_source, fake_dest)\n",
    "        \n",
    "        print(f\"✓ Copied {true_file_candidates[0]} to {true_dest}\")\n",
    "        print(f\"✓ Copied {fake_file_candidates[0]} to {fake_dest}\")\n",
    "        \n",
    "        # Set file paths for later use\n",
    "        true_file_path = true_dest\n",
    "        fake_file_path = fake_dest\n",
    "        dataset_ready = True\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Could not find True.csv and Fake.csv files\")\n",
    "        print(\"Available files:\", downloaded_files)\n",
    "        dataset_ready = False\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"❌ kagglehub not installed. Please install it with:\")\n",
    "    print(\"pip install kagglehub\")\n",
    "    dataset_ready = False\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error downloading dataset: {e}\")\n",
    "    print(\"\\nAlternative: Download manually from:\")\n",
    "    print(\"https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\")\n",
    "    print(\"Then place True.csv and Fake.csv in the ./data directory\")\n",
    "    dataset_ready = False\n",
    "\n",
    "# Check if files exist in data directory\n",
    "true_file_path = os.path.join(data_dir, \"True.csv\")\n",
    "fake_file_path = os.path.join(data_dir, \"Fake.csv\")\n",
    "\n",
    "if os.path.exists(true_file_path) and os.path.exists(fake_file_path):\n",
    "    dataset_ready = True\n",
    "    print(f\"✓ Dataset files ready:\")\n",
    "    print(f\"  - {true_file_path}\")\n",
    "    print(f\"  - {fake_file_path}\")\n",
    "else:\n",
    "    print(f\"❌ Dataset files not found in {data_dir}\")\n",
    "    dataset_ready = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5630a",
   "metadata": {},
   "source": [
    "## Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b07918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess a given text string by normalizing, tokenizing, \n",
    "    removing stop words, and lemmatizing.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed text string\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]  # Remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]   # Lemmatize\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5297d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalize column names by stripping whitespace and converting to lowercase.\"\"\"\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6d6be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_date_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find a likely date column, parse to datetime, and write back to a unified 'date' column.\n",
    "    If no date-like column exists, create an empty 'date' column.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        \"date\", \"published\", \"publish_date\", \"publication_date\",\n",
    "        \"pub_date\", \"created_at\", \"time\", \"timestamp\"\n",
    "    ]\n",
    "    date_col = next((c for c in candidates if c in df.columns), None)\n",
    "    \n",
    "    if date_col is None:\n",
    "        df[\"date\"] = pd.NaT\n",
    "    else:\n",
    "        df[\"date\"] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    \n",
    "    # Write dates as ISO strings (YYYY-MM-DD); keep NaN if unknown\n",
    "    df[\"date\"] = df[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6bd9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebbfe10",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fca33b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'True.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the datasets\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_true = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTrue.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m df_fake = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mFake.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Normalize headers to avoid case/space issues\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ai520_venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ai520_venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ai520_venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ai520_venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ai520_venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'True.csv'"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "df_true = pd.read_csv(\"True.csv\")\n",
    "df_fake = pd.read_csv(\"Fake.csv\")\n",
    "\n",
    "# Normalize headers to avoid case/space issues\n",
    "df_true = normalize_columns(df_true)\n",
    "df_fake = normalize_columns(df_fake)\n",
    "\n",
    "# Make sure we have a usable 'date' column in both\n",
    "df_true = ensure_date_column(df_true)\n",
    "df_fake = ensure_date_column(df_fake)\n",
    "\n",
    "print(\"Unprocessed True dataset shape:\", df_true.shape)\n",
    "print(\"Unprocessed Fake dataset shape:\", df_fake.shape)\n",
    "print(\"\\nTrue dataset columns:\", df_true.columns.tolist())\n",
    "print(\"Fake dataset columns:\", df_fake.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c8850",
   "metadata": {},
   "source": [
    "## Data Validation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffd8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guard for missing title/text columns\n",
    "required = {\"title\", \"text\"}\n",
    "missing_true = required - set(df_true.columns)\n",
    "missing_fake = required - set(df_fake.columns)\n",
    "\n",
    "if missing_true:\n",
    "    raise KeyError(f\"True.csv missing required columns: {missing_true}\")\n",
    "if missing_fake:\n",
    "    raise KeyError(f\"Fake.csv missing required columns: {missing_false}\")\n",
    "\n",
    "# Apply preprocessing to title and text columns\n",
    "print(\"Preprocessing text data...\")\n",
    "df_true[\"processed_title\"] = df_true[\"title\"].astype(str).apply(preprocess_text)\n",
    "df_fake[\"processed_title\"] = df_fake[\"title\"].astype(str).apply(preprocess_text)\n",
    "df_true[\"processed_text\"] = df_true[\"text\"].astype(str).apply(preprocess_text)\n",
    "df_fake[\"processed_text\"] = df_fake[\"text\"].astype(str).apply(preprocess_text)\n",
    "\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893eb283",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data and Display Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269af65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data\n",
    "df_true[[\"processed_title\", \"processed_text\", \"date\"]].to_csv(\"preprocessed_True.csv\", index=False)\n",
    "df_fake[[\"processed_title\", \"processed_text\", \"date\"]].to_csv(\"preprocessed_Fake.csv\", index=False)\n",
    "\n",
    "print(\"Processed True dataset sample:\")\n",
    "print(df_true[[\"processed_title\", \"processed_text\", \"date\"]].head())\n",
    "print(\"\\nProcessed Fake dataset sample:\")\n",
    "print(df_fake[[\"processed_title\", \"processed_text\", \"date\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69315290",
   "metadata": {},
   "source": [
    "## Prepare Data for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84dedd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "df_fake = pd.read_csv(\"preprocessed_Fake.csv\")\n",
    "df_true = pd.read_csv(\"preprocessed_True.csv\")\n",
    "\n",
    "# Label the data\n",
    "df_fake['label'] = 0  # fake news\n",
    "df_true['label'] = 1  # true news\n",
    "\n",
    "# Combine the datasets\n",
    "df_combined = pd.concat([df_fake, df_true], ignore_index=True)\n",
    "\n",
    "# Combine title and text for feature extraction\n",
    "df_combined['combined'] = df_combined['processed_title'].fillna('') + ' ' + df_combined['processed_text'].fillna('')\n",
    "\n",
    "print(f\"Combined dataset shape: {df_combined.shape}\")\n",
    "print(f\"Label distribution:\\n{df_combined['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1d4ad",
   "metadata": {},
   "source": [
    "## Feature Extraction with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform data\n",
    "X = vectorizer.fit_transform(df_combined['combined'])  # Feature matrix\n",
    "y = df_combined['label']  # Target labels\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", X.shape)\n",
    "print(\"Sample features:\", vectorizer.get_feature_names_out()[:20])\n",
    "print(\"Feature matrix sparsity:\", (X != 0).nnz / (X.shape[0] * X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f0cbf",
   "metadata": {},
   "source": [
    "## Classifier 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c6370",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LOGISTIC REGRESSION CLASSIFIER\")\n",
    "\n",
    "# Create training/testing split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f82b83",
   "metadata": {},
   "source": [
    "## Classifier 2 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e02f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RANDOM FOREST CLASSIFIER\")\n",
    "\n",
    "# Create training/testing split (same random state for fair comparison)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,       # number of trees\n",
    "    random_state=42,        # for reproducibility\n",
    "    n_jobs=-1               # use all CPU cores\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(\"Random Forest Results:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28b6e33",
   "metadata": {},
   "source": [
    "## Classifier 3 - Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a797b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LINEAR SVM CLASSIFIER\")\n",
    "\n",
    "# Create training/testing split (same random state for fair comparison)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Linear SVM pipeline with scaling\n",
    "svm_model = make_pipeline(\n",
    "    StandardScaler(with_mean=False),  # Don't center sparse matrices\n",
    "    LinearSVC(dual=\"auto\", C=1.0, tol=1e-3, max_iter=5000, random_state=42)\n",
    ")\n",
    "\n",
    "# Train model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print(\"Linear SVM Results:\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9265756a",
   "metadata": {},
   "source": [
    "## Model Comparison and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbce668",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"All models trained on the same train/test split (random_state=42)\")\n",
    "print(\"Dataset size:\", df_combined.shape[0], \"samples\")\n",
    "print(\"Feature dimensions:\", X.shape[1], \"TF-IDF features\")\n",
    "print(\"Train/Test split: 80/20\")\n",
    "print(\"\\nRerun the classification cells above to see detailed results for each model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai520_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
